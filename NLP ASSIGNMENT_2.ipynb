{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are Corpora?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a corpus?\n",
    "A text corpus is a very large collection of text (often many billion words) produced by real users of the language and used to analyse how words, phrases and language in general are used. It is used by linguists, lexicographers, social scientists, humanities, experts in natural language processing and in many other fields. A corpus is also be used for generating various language databases used in software development such as predictive keyboards, spell check, grammar correction, text/speech understanding systems, text-to-speech modules, machine translation systems and many others.\n",
    "\n",
    "Types of text corpora\n",
    "It is not possible to easily classify a corpus into a certain category. Instead, corpora can have features or properties which can be used to group them. The same corpus can have one or more of these features.\n",
    "\n",
    "Language\n",
    "Monolingual corpus\n",
    "A monolingual corpus is the most frequent type of corpus. It contains texts in one language only. The corpus is usually tagged for parts of speech and is used by a wide range of users for various tasks from highly practical ones, e.g. checking the correct usage of a word or looking up the most natural word combinations, to scientific use, e.g. identifying frequent patterns or new trends in language. Sketch Engine contains hundreds of monolingual corpora in dozens of languages.\n",
    "\n",
    "see also What can Sketch Engine do? and Build your own corpus\n",
    "\n",
    "Parallel corpus, multilingual corpus\n",
    "A parallel corpus consists of two or more monolingual corpora. The corpora are the translations of each other.  For example, a novel and its translation or a translation memory of a CAT tool could be used to build a parallel corpus. Both languages need to be aligned, i.e. corresponding segments, usually sentences or paragraphs, need to be matched. The user can then search for all examples of a word or phrase in one language and the results will be displayed together with the corresponding sentences in the other language. The user can then observe how the search word or phrase is translated.\n",
    "\n",
    "see also Parallel / Bilingual Concordance and Build a parallel corpus\n",
    "\n",
    "Comparable corpus\n",
    "A comparable corpus is one corpus in a set of two or more monolingual corpora, typically each in a different language, built according to the same principles. The content is therefore similar and results can be compared between the corpora even though they are not translations of each other (and therefore, there are not aligned). When users search these corpora they can use the fact, that the corpora also have the same metadata. An example of comparable corpora in Sketch Engine is CHILDES corpora or various corpora made from Wikipedia. Araneum corpora are comparable too.\n",
    "\n",
    "see comparable corpora CHILDES corpora and corpora from Wikipedia\n",
    "\n",
    "Time\n",
    "Diachronic corpus\n",
    "A diachronic corpus is a corpus containing texts from different periods and is used to study the development or change in language. Sketch Engine allows searching the corpus as a whole or only include selected time intervals into the search. In addition, there is a specialized diachronic feature called Trends, which identifies words whose usage changes the most of the selected period of time.\n",
    "\n",
    "see also Trends – diachronic analysis\n",
    "\n",
    "Synchronic corpus\n",
    "The opposite is a synchronic corpus whose texts come from the same point of time. It is a snapshot of language in one moment. The enTenTen family of corpora are such snapshots because their content is collected within a couple of months.\n",
    "\n",
    "Currentness\n",
    "Static corpus\n",
    "(also called a reference corpus (although this refers to something else in Sketch Engine) is a corpus whose development is complete. The content of the corpus does not change. Most corpora are static corpora. The benefit of a corpus that does not change is that the results of the analysis do not change which is important in many scenarios.\n",
    "\n",
    "Monitor corpus\n",
    "A monitor corpus is used to monitor the change in language. It is a corpus which is regularly (or even continuously) updated, new texts are added as they are produced. The results of the searches change because the content of the corpus gets bigger all the time.\n",
    "\n",
    "The Timestamped corpus in Sketch Engine is an example of a monitor corpus.\n",
    "\n",
    "More features\n",
    "Learner corpora\n",
    "A learner corpus is a corpus of texts produced by learners of a language. The corpus is used to study the mistakes and problems learners have when learning a foreign language. Sketch Engine allows for learner corpora to be annotated for the type of error and provides a special interface to search either for the error itself, for the error correction, for the error type or for a combination of the three options.\n",
    "\n",
    "see also Setting up a learner corpus\n",
    "\n",
    "Error-annotated corpus\n",
    "These corpora contain texts produced by learners of a language or by translators. The  errors are annotated and can be used to study the types of errors diferent groups of learners or translators make.\n",
    "\n",
    "see also Setting up a learner corpus\n",
    "\n",
    "Specialized corpus\n",
    "A specialized corpus contains texts limited to one or more subject areas, domains, topics etc. Such corpus is used to study how the specialized language is used. The user can create specialized subcorpora from the general corpora in Sketch Engine.\n",
    "\n",
    "see Build a subcorpus\n",
    "\n",
    "Multimedia corpus\n",
    "A multimedia corpus contains texts which are enhanced with audio or visual materials or other type of multimedia content. For example, the spoken part of British National Corpus in Sketch Engine has links to the corresponding recordings which can be played from the Sketch Engine interface.\n",
    "\n",
    "Other corpora can have videos where the corpus text is spoken or images which show the original manuscript or printed copy of the text.\n",
    "\n",
    "See BNC, where the spoken part is also available in the audio format and it can be played directly in the Sketch Engine interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are Tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are the building blocks of Natural Language. Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are Unigrams, Bigrams, Trigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Grams are phrases cut out of a sentence with N cinsecutive words. Thus a Unigram takes a sentence and gives us all the words in that we fence. A Bigram takes a sentence and gives us sets of two consecutive words in the sentence. A Trigram gives sets of threee consecutive words in a sentence.\n",
    "\n",
    "Let me explain with an example.\n",
    "\n",
    "Unigram - [Let] [me] [explain] [with] [an] [example.]\n",
    "\n",
    "Bigram [let me] [me explain] [explain with] [with an] [an example]\n",
    "\n",
    "Trigram [let me explain] [me explain with] [explain with an] [with an example]\n",
    "\n",
    "Hope it explains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to generate n-grams from text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\n",
    "\n",
    "When performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents. This post describes several different ways to generate n-grams quickly from input sentences in Python.\n",
    "\n",
    "The Pure Python Way\n",
    "In general, an input sentence is just a string of characters in Python. We can use build in functions in Python to generate n-grams quickly. Let’s take the following sentence as a sample input:\n",
    "\n",
    "s = \"\"\"\n",
    "    Natural-language processing (NLP) is an area of\n",
    "    computer science and artificial intelligence\n",
    "    concerned with the interactions between computers\n",
    "    and human (natural) languages.\n",
    "\"\"\"\n",
    "If we want to generate a list of bi-grams from the above sentence, the expected output would be something like below (depending on how do we want to treat the punctuations, the desired output can be different):\n",
    "\n",
    "[\n",
    "    \"natural language\",\n",
    "    \"language processing\",\n",
    "    \"processing nlp\",\n",
    "    \"nlp is\",\n",
    "    \"is an\",\n",
    "    \"an area\",\n",
    "    ...\n",
    "]\n",
    "The following function can be used to achieve this:\n",
    "\n",
    "import re\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[token[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "Applying the above function to the sentence, with n=5, gives the following output:\n",
    "\n",
    ">>> generate_ngrams(s, n=5)\n",
    "['natural language processing nlp is',\n",
    " 'language processing nlp is an',\n",
    " 'processing nlp is an area',\n",
    " 'nlp is an area of',\n",
    " 'is an area of computer',\n",
    " 'an area of computer science',\n",
    " 'area of computer science and',\n",
    " 'of computer science and artificial',\n",
    " 'computer science and artificial intelligence',\n",
    " 'science and artificial intelligence concerned',\n",
    " 'and artificial intelligence concerned with',\n",
    " 'artificial intelligence concerned with the',\n",
    " 'intelligence concerned with the interactions',\n",
    " 'concerned with the interactions between',\n",
    " 'with the interactions between computers',\n",
    " 'the interactions between computers and',\n",
    " 'interactions between computers and human',\n",
    " 'between computers and human natural',\n",
    " 'computers and human natural languages']\n",
    "The above function makes use of the zip function, which creates a generator that aggregates elements from multiple lists (or iterables in genera). The blocks of codes and comments below offer some more explanation of the usage:\n",
    "\n",
    "# Sample sentence\n",
    "s = \"one two three four five\"\n",
    "\n",
    "tokens = s.split(\" \")\n",
    "# tokens = [\"one\", \"two\", \"three\", \"four\", \"five\"]\n",
    "\n",
    "sequences = [tokens[i:] for i in range(3)]\n",
    "# The above will generate sequences of tokens starting\n",
    "# from different elements of the list of tokens.\n",
    "# The parameter in the range() function controls\n",
    "# how many sequences to generate.\n",
    "#\n",
    "# sequences = [\n",
    "#   ['one', 'two', 'three', 'four', 'five'],\n",
    "#   ['two', 'three', 'four', 'five'],\n",
    "#   ['three', 'four', 'five']]\n",
    "\n",
    "bigrams = zip(*sequences)\n",
    "# The zip function takes the sequences as a list of inputs\n",
    "# (using the * operator, this is equivalent to\n",
    "# zip(sequences[0], sequences[1], sequences[2]).\n",
    "# Each tuple it returns will contain one element from\n",
    "# each of the sequences.\n",
    "# \n",
    "# To inspect the content of bigrams, try:\n",
    "# print(list(bigrams))\n",
    "# which will give the following:\n",
    "#\n",
    "# [\n",
    "#   ('one', 'two', 'three'),\n",
    "#   ('two', 'three', 'four'),\n",
    "#   ('three', 'four', 'five')\n",
    "# ]\n",
    "#\n",
    "# Note: even though the first sequence has 5 elements,\n",
    "# zip will stop after returning 3 tuples, because the\n",
    "# last sequence only has 3 elements. In other words,\n",
    "# the zip function automatically handles the ending of\n",
    "# the n-gram generation.\n",
    "Using NLTK\n",
    "Instead of using pure Python functions, we can also get help from some natural language processing libraries such as the Natural Language Toolkit (NLTK). In particular, nltk has the ngrams function that returns a generator of n-grams given a tokenized sentence. (See the documentaion of the function here)\n",
    "\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 5))\n",
    "The above block of code will generate the same output as the function generate_ngrams() as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a linguistic term that means grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item. The aim is to take away inflectional suffixes and prefixes to bring out the word’s dictionary form.\n",
    "\n",
    "For example, to lemmatize the words “cats,” “cat’s,” and “cats’” means taking away the suffixes “s,” “’s,” and “s’” to bring out the root word “cat.” Lemmatization is used to train robots to speak and converse, making it important in the field of artificial intelligence (AI) known as “natural language processing (NLP)” or “natural language understanding.”\n",
    "\n",
    "Other interesting terms…\n",
    "\n",
    "What is Natural Language Processing?\n",
    "What is Machine Learning?\n",
    "Read More about “Lemmatization”\n",
    "In general, lemmatization converts words into their base forms. In linguistics, lemmatization helps a reader consider a word’s intended meaning instead of its literal meaning. Because of that, lemmatization is often confused with stemming.\n",
    "\n",
    "Differences between Lemmatization and Stemming\n",
    "In stemming, a computer algorithm often cuts off the ending or beginning of the word being analyzed. The cut thus takes out prefixes and suffixes, which can lead to errors. Let’s take the words “studies” as an example. A stemming algorithm would drop the suffix “es,” thus arriving at the root word “studi,” which we all know is not right. There’s no such word.\n",
    "\n",
    "Lemmatization, on the other hand, lets a word like “studies” undergo a morphological analysis based on a dictionary that the algorithm can consult to produce the correct root word. As such, a lemmatization-capable machine would know that “studies” is the singular verb form of the word “study” in the present tense.\n",
    "\n",
    "Practical Applications of Lemmatization\n",
    "As we said earlier, lemmatization is a crucial component of NLP. It is widely applied in text mining, which involves text analysis of data written in the natural language. This process allows computers to extract relevant information from a given set of text.\n",
    "\n",
    "One widely known application of lemmatization is information retrieval for search engines. Lemmatization allows systems to map documents to topics, allowing search engines to display relevant results and even expanding them to include other information that readers may find useful, too.\n",
    "\n",
    "Lemmatization is also used in sentiment analysis, which includes text preparation before examination. The concept is also applied in document clustering, where users need to extract topics and retrieve information.\n",
    "\n",
    "Lemmatization is also useful in improving search engine optimization (SEO) results. Search engines like Google employ the technology to provide highly relevant results to users. Note that when users type in queries, a search engine automatically lemmatizes words to make sense of the search term and give relevant and comprehensive results.\n",
    "\n",
    "Some examples of lemmatization tools currently out in the market include:\n",
    "\n",
    "BioLemmatizer: Helps computers make sense of biomedical literature.\n",
    "Lemmatization API: Automatically obtains the root of any given word.\n",
    "Trinker/Textstem: Functions much like Lemmatization API.\n",
    "—\n",
    "\n",
    "Lemmatization, in a nutshell, is the process of obtaining the root of any word to make sense of a phrase, clause, sentence, or any kind of content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Explain Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\n",
    "\n",
    "Stemming is a part of linguistic studies in morphology and artificial intelligence (AI) information retrieval and extraction. Stemming and AI knowledge extract meaningful information from vast sources like big data or the Internet since additional forms of a word related to a subject may need to be searched to get the best results. Stemming is also a part of queries and Internet search engines.\n",
    "\n",
    "Recognizing, searching and retrieving more forms of words returns more results. When a form of a word is recognized it can make it possible to return search results that otherwise might have been missed. That additional information retrieved is why stemming is integral to search queries and information retrieval.\n",
    "\n",
    "When a new word is found, it can present new research opportunities. Often, the best results can be attained by using the basic morphological form of the word: the lemma. To find the lemma, stemming is performed by an individual or an algorithm, which may be used by an AI system. Stemming uses a number of approaches to reduce a word to its base from whatever inflected form is encountered.\n",
    "\n",
    "It can be simple to develop a stemming algorithm. Some simple algorithms will simply strip recognized prefixes and suffixes. However, these simple algorithms are prone to error. For example, an error can reduce words like laziness to lazi  instead of lazy. Such algorithms may also have difficulty with terms whose inflectional forms don't perfectly mirror the lemma such as with saw and see.\n",
    "\n",
    "Examples of stemming algorithms include:\n",
    "\n",
    "Lookups in tables of inflected forms of words. This approach requires all inflected forms be listed.\n",
    "\n",
    "Suffix strippi . Algorithms recognize known suffixes on inflected words and remove them.\n",
    "\n",
    "Lemmatization. This algorithm collects all inflected forms of a word in order to break them down to their root dictionary form or lemma. Words are broken down into a part of speech (the categories of word types) by way of the rules of grammar.\n",
    "\n",
    "Stochastic models. This algorithm earns from tables of inflected forms of words. By understanding suffixes, and the rules by which they are applied, an algorithm can stem new words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Explain Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag in case of is a part-of-speech tag, and signifies whether the word is a noun, adjective, verb, and so on.\n",
    "\n",
    "Default tagging is a basic step for the part-of-speech tagging. It is performed using the DefaultTagger class. The DefaultTagger class takes ‘tag’ as a single argument. NN is the tag for a singular noun. DefaultTagger is most useful when it gets to work with most common part-of-speech tag. that’s why a noun tag is recommended.\n",
    "\n",
    "\n",
    "\n",
    "Code #1 : How it works ?\n",
    "\n",
    "# Loading Libraries\n",
    "from nltk.tag import DefaultTagger\n",
    "  \n",
    "# Defining Tag\n",
    "tagging = DefaultTagger('NN')\n",
    "  \n",
    "# Tagging\n",
    "tagging.tag(['Hello', 'Geeks'])\n",
    "Output :\n",
    "\n",
    "[('Hello', 'NN'), ('Geeks', 'NN')]\n",
    "Each tagger has a tag() method that takes a list of tokens (usually list of words produced by a word tokenizer), where each token is a single word. tag() returns a list of tagged tokens – a tuple of (word, tag).\n",
    "\n",
    "\n",
    "\n",
    "How DefaultTagger works ?\n",
    "It is a subclass of SequentialBackoffTagger and implements the choose_tag() method, having three arguments.\n",
    "\n",
    "list of tokens\n",
    "index of the current token, to choose the tag.\n",
    "list of the previous tags\n",
    " \n",
    "Code #2 : Tagging Sentences\n",
    "\n",
    "# Loading Libraries\n",
    "from nltk.tag import DefaultTagger\n",
    "  \n",
    "# Defining Tag\n",
    "tagging = DefaultTagger('NN')\n",
    "  \n",
    "tagging.tag_sents([['welcome', 'to', '.'], ['Geeks', 'for', 'Geeks']])\n",
    "Output :\n",
    "\n",
    "[[('welcome', 'NN'), ('to', 'NN'), ('.', 'NN')],\n",
    " [('Geeks', 'NN'), ('for', 'NN'), ('Geeks', 'NN')]]\n",
    "Note: Every tag in the list of tagged sentences (in the above code) is NN as we have used DefaultTagger class.\n",
    "\n",
    "Code #3 : Illustrating how to untag.\n",
    "\n",
    "from nltk.tag import untag\n",
    "untag([('Geeks', 'NN'), ('for', 'NN'), ('Geeks', 'NN')])\n",
    "Output :\n",
    "\n",
    "['Geeks', 'for', 'Geeks']\n",
    " Attention geek! Strengthen your foundations with the Python Programming Foundation Course and learn the basics.  \n",
    "\n",
    "To begin with, your interview preparations Enhance your Data Structures concepts with the Python DS Course. And to begin with your Machine Learning Journey, join the Machine Learning – Basic Level Course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Explain Chunking or shallow parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking (Shallow Parsing): Understanding Text Syntax and Structures, Part 2\n",
    "We got introduced to text syntax and structures and took a detailed look at part of speech tagging in part 1 of this tutorial series. In this tutorial, we will learn about phrasal structure and shallow parsing.\n",
    "\n",
    "Phrasal Structures\n",
    "A phrase can be a single word or a combination of words based on the syntax and position of the phrase in a clause or sentence. For example, in the following sentence\n",
    "\n",
    "My dog likes his food.\n",
    "there are three phrases. \"My dog\" is a noun phrase, \"likes\" is a verb phrase, and \"his food\" is also a noun phrase.\n",
    "\n",
    "There are five major categories of phrases:\n",
    "\n",
    "Noun phrase (NP): These are phrases where a noun acts as the head word. Noun phrases act as a subject or object to a verb or an adjective. In some cases a noun phrase can be replaced by a pronoun without changing the syntax of the sentence. Some examples of Noun phrases are \"little boy\", \"hard rock\", etc.\n",
    "Verb phrase (VP): These phrases are lexical units that have a verb acting as the head word. Usually there are two forms of verb phrases. One form has the verb components as well as other entities such as nouns, adjectives, or adverbs as parts of the object. The verb here is known as a finite verb. For example in the sentence “The boy is playing football”, \"playing football\" is the finite verb phrase. The second form of this includes verb phrases which consist strictly of verb components only. For example, \"is playing\" in the same sentence is such a verb phrase.\n",
    "Adjective phrase (ADJP): These are phrases with an adjective as the head word. Their main role is to describe or qualify nouns and pronouns in a sentence, and they will be either placed before or after the noun or pronoun. The sentence, \"The cat is too cute\" has an adjective phrase, \"too cute\", qualifying \"cat\".\n",
    "Adverb phrase (ADVP): These are phrases where adverb acts as the head word in the phrase. Adverb phrases are used as modifiers for nouns, verbs, or adverbs themselves by providing further details that describe or qualify them. In the sentence \"The train should be at the station pretty soon\", the adverb phrase \"pretty soon\" describes when the train would be arriving.\n",
    "Prepositional phrase (PP): These phrases usually contain a preposition as the head word and other lexical components like nouns, pronouns, and so on. It acts like an adjective or adverb describing other words or phrases. The phrase \"going up the stairs\" contains a prepositional phrase \"up\", describing the direction of the stairs.\n",
    "These five major syntactic categories of phrases can be generated from words using several rules, utilizing syntax and grammars of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Explain Noun Phrase (NP) chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Explain Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition (NER) — sometimes referred to as entity chunking, extraction, or identification — is the task of identifying and categorizing key information (entities) in text. An entity can be any word or series of words that consistently refers to the same thing. Every detected entity is classified into a predetermined category. For example, an NER machine learning (ML) model might detect the word “super.AI” in a text and classify it as a “Company”.\n",
    "NER is a form of natural language processing (NLP), a subfield of artificial intelligence. NLP is concerned with computers processing and analyzing natural language, i.e., any language that has developed naturally, rather than artificially, such as with computer coding languages.\n",
    "This post explores the basics of how NER works, along with some high-level use cases and how you can apply it in your business or project.\n",
    "\n",
    "NER does not evaluate the truth of statements\n",
    "How NER works\n",
    "At the heart of any NER model is a two step process:\n",
    "Detect a named entity\n",
    "Categorize the entity\n",
    "Beneath this lie a couple of things.\n",
    "Step one involves detecting a word or string of words that form an entity. Each word represents a token: “The Great Lakes” is a string of three tokens that represents one entity. Inside-outside-beginning tagging is a common way of indicating where entities begin and end. We’ll explore this further in a future blog post.\n",
    "The second step requires the creation of entity categories. Here are some common entity categories:\n",
    "Person\n",
    "E.g., Elvis Presley, Audrey Hepburn, David Beckham\n",
    "Organization\n",
    "E.g., Google, Mastercard, University of Oxford\n",
    "Time\n",
    "E.g., 2006, 16:34, 2am\n",
    "Location\n",
    "E.g., Trafalgar Square, MoMA, Machu Picchu\n",
    "Work of art\n",
    "E.g., Hamlet, Guernica, Exile on Main St.\n",
    "These are just a few examples. You can create your own entity categories to suit your task, as well as provide granular rules for which entities belong to which categories in instances of ambiguity or task-specific ontologies.\n",
    "\n",
    "super.AI’s interface allows you to decide your entities\n",
    "To learn what is and is not a relevant entity and how to categorize them, a model requires training data. The more relevant that training data is to the task, the more accurate the model will be at completing said task. Train your model on Victorian gothic literature, and it will probably struggle to navigate Twitter.\n",
    "Once you have defined your entities and your categories, you can use these to label data and create a training dataset (our named entity recognition data program can do this for you automatically). You then use this training dataset to train an algorithm to label your text predictively.\n",
    "How is NER used?\n",
    "NER is suited to any situation in which a high-level overview of a large quantity of text is helpful. With NER, you can, at a glance, understand the subject or theme of a body of text and quickly group texts based on their relevancy or similarity.\n",
    "Some notable NER use cases include:\n",
    "Human resources\n",
    "Speed up the hiring process by summarizing applicants’ CVs; improve internal workflows by categorizing employee complaints and questions\n",
    "Customer support\n",
    "Improve response times by categorizing user requests, complaints and questions and filtering by priority keywords\n",
    "Search and recommendation engines\n",
    "Improve the speed and relevance of search results and recommendations by summarizing descriptive text, reviews, and discussions\n",
    "Booking.com is a notable success story here\n",
    "Content classification\n",
    "Surface content more easily and gain insights into trends by identifying the subjects and themes of blog posts and news articles\n",
    "Health care\n",
    "Improve patient care standards and reduce workloads by extracting essential information from lab reports\n",
    "Roche is doing this with pathology and radiology reports\n",
    "Academia\n",
    "Enable students and researchers to find relevant material faster by summarizing papers and archive material and highlighting key terms, topics, and themes\n",
    "The EU’s digital platform for cultural heritage, Europeana, is using NER to make historical newspapers searchable\n",
    "\n",
    "Wherever there are large quantities of text, NER can make life easier\n",
    "How can I use NER?\n",
    "If you think that your business or project could benefit from NER, it’s pretty easy to start out. There are a number of excellent open-source libraries that can get you going, including NLTK, SpaCy, and Stanford NER. Each has its own pros and cons, which we’ll be exploring in more detail soon.\n",
    "But before you begin using one of these libraries to build a model, you will need to produce a relevant labeled dataset to train the model on. That’s where super.AI is there to help. Using our named entity recognition data program, you provide us your raw text and desired entities and categories. We’ll label the text you send and return a high quality training dataset that you can take to train and tailor your NER model.\n",
    "If you’re interested in learning more or have a specialized use case, reach out to us. You can also stay tuned to our blog, where we’ll be running a series of posts covering different aspects of NLP over the coming months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
