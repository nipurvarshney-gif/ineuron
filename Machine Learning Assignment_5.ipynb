{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the key tasks that machine learning entails? What does data pre-processing imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing includes cleaning, Instance selection, normalization, transformation, feature extraction and selection, etc. The product of data preprocessing is the final training set. Data preprocessing may affect the way in which outcomes of the final data processing can be interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps Involved in Data Preprocessing:\n",
    "\n",
    "1. Data Cleaning:\n",
    "The data can have many irrelevant and missing parts. To handle this part, data cleaning is done. It involves handling of missing data, noisy data etc.\n",
    "\n",
    "(a). Missing Data:\n",
    "This situation arises when some data is missing in the data. It can be handled in various ways.\n",
    "Some of them are:\n",
    "Ignore the tuples:\n",
    "This approach is suitable only when the dataset we have is quite large and multiple values are missing within a tuple.\n",
    "Fill the Missing values:\n",
    "There are various ways to do this task. You can choose to fill the missing values manually, by attribute mean or the most probable value.\n",
    "(b). Noisy Data:\n",
    "Noisy data is a meaningless data that can’t be interpreted by machines.It can be generated due to faulty data collection, data entry errors etc. It can be handled in following ways :\n",
    "\n",
    "Binning Method:\n",
    "This method works on sorted data in order to smooth it. The whole data is divided into segments of equal size and then various methods are performed to complete the task. Each segmented is handled separately. One can replace all data in a segment by its mean or boundary values can be used to complete the task.\n",
    "Regression:\n",
    "Here data can be made smooth by fitting it to a regression function.The regression used may be linear (having one independent variable) or multiple (having multiple independent variables).\n",
    "Clustering:\n",
    "This approach groups the similar data in a cluster. The outliers may be undetected or it will fall outside the clusters.\n",
    "2. Data Transformation:\n",
    "This step is taken in order to transform the data in appropriate forms suitable for mining process. This involves following ways:\n",
    "\n",
    "Normalization:\n",
    "It is done in order to scale the data values in a specified range (-1.0 to 1.0 or 0.0 to 1.0)\n",
    "Attribute Selection:\n",
    "In this strategy, new attributes are constructed from the given set of attributes to help the mining process.\n",
    "Discretization:\n",
    "This is done to replace the raw values of numeric attribute by interval levels or conceptual levels.\n",
    "Concept Hierarchy Generation:\n",
    "Here attributes are converted from level to higher level in hierarchy. For Example-The attribute “city” can be converted to “country”.\n",
    "3. Data Reduction:\n",
    "Since data mining is a technique that is used to handle huge amount of data. While working with huge volume of data, analysis became harder in such cases. In order to get rid of this, we uses data reduction technique. It aims to increase the storage efficiency and reduce data storage and analysis costs.\n",
    "\n",
    "The various steps to data reduction are:\n",
    "\n",
    "Data Cube Aggregation:\n",
    "Aggregation operation is applied to data for the construction of the data cube.\n",
    "Attribute Subset Selection:\n",
    "The highly relevant attributes should be used, rest all can be discarded. For performing attribute selection, one can use level of significance and p- value of the attribute.the attribute having p-value greater than significance level can be discarded.\n",
    "Numerosity Reduction:\n",
    "This enable to store the model of data instead of whole data, for example: Regression Models.\n",
    "Dimensionality Reduction:\n",
    "This reduce the size of data by encoding mechanisms.It can be lossy or lossless. If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction else it is called lossy reduction. The two effective methods of dimensionality reduction are:Wavelet transforms and PCA (Principal Componenet Analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Describe quantitative and qualitative data in depth. Make a distinction between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative research deals with numbers and statistics, while qualitative research deals with words and meanings. Quantitative methods allow you to test a hypothesis by systematically collecting and analyzing data, while qualitative methods allow you to explore ideas and experiences in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative research\n",
    "Focuses on testing theories and hypotheses\n",
    "Analyzed through math and statistical analysis\n",
    "Mainly expressed in numbers, graphs and tables\n",
    "Requires many respondents\n",
    "Closed (multiple choice) questions\n",
    "Key terms: testing, measurement, objectivity, replicability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qualitative Research\n",
    "Focuses on exploring ideas and formulating a theory or hypothesis\n",
    "Analyzed by summarizing, categorizing and interpreting\n",
    "Mainly expressed in words\n",
    "Requires few respondents\n",
    "Open-ended questions\n",
    "Key terms: understanding, context, complexity, subjectivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a basic data collection that includes some sample records. Have at least one attribute from\n",
    "each of the machine learning data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What are the various causes of machine learning data issues? What are the ramifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Understanding Which Processes Need Automation\n",
    "It's becoming increasingly difficult to separate fact from fiction in terms of Machine Learning today. Before you decide on which AI platform to use, you need to evaluate which problems you’re seeking to solve. The easiest processes to automate are the ones that are done manually every day with no variable output. Complicated processes require further inspection before automation. While Machine Learning can definitely help automate some processes, not all automation problems need Machine Learning.\n",
    "\n",
    "Caution bad data warning sign\n",
    "\n",
    "2) Lack of Quality Data\n",
    "The number one problem facing Machine Learning is the lack of good data. While enhancing algorithms often consumes most of the time of developers in AI, data quality is essential for the algorithms to function as intended. Noisy data, dirty data, and incomplete data are the quintessential enemies of ideal Machine Learning. The solution to this conundrum is to take the time to evaluate and scope data with meticulous data governance, data integration, and data exploration until you get clear data. You should do this before you start.\n",
    "\n",
    "3) Inadequate Infrastructure\n",
    "cables plugged into server\n",
    "\n",
    "Machine Learning requires vast amounts of data churning capabilities. Legacy systems often can’t handle the workload and buckle under pressure. You should check if your infrastructure can handle Machine Learning. If it can’t, you should look to upgrade, complete with hardware acceleration and flexible storage.\n",
    "\n",
    "4) Implementation\n",
    "Organizations often have analytics engines working with them by the time they choose to upgrade to Machine Learning. Integrating newer Machine Learning methodologies into existing methodologies is a complicated task. Maintaining proper interpretation and documentation goes a long way to easing implementation. Partnering with an implementation partner can make the implementation of services like anomaly detection, predictive analysis, and ensemble modeling much easier.\n",
    "\n",
    "5) Lack of Skilled Resources\n",
    "skills word map \n",
    "\n",
    "Deep analytics and Machine Learning in their current forms are still new technologies. Thus, there is a shortage of skilled employees available to manage and develop analytical content for Machine Learning. Data scientists often need a combination of domain experience as well as in-depth knowledge of science, technology, and mathematics. Recruitment will require you to pay large salaries as these employees are often in high-demand and know their worth. You can also approach your vendor for staffing help as many managed service providers keep a list of skilled data scientists to deploy anytime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Demonstrate various approaches to categorical data exploration with appropriate examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Challenge With Categorical Data\n",
    "A categorical variable is a variable whose values take on the value of labels.\n",
    "\n",
    "For example, the variable may be “color” and may take on the values “red,” “green,” and “blue.”\n",
    "\n",
    "Sometimes, the categorical data may have an ordered relationship between the categories, such as “first,” “second,” and “third.” This type of categorical data is referred to as ordinal and the additional ordering information can be useful.\n",
    "\n",
    "Machine learning algorithms and deep learning neural networks require that input and output variables are numbers.\n",
    "\n",
    "This means that categorical data must be encoded to numbers before we can use it to fit and evaluate a model.\n",
    "\n",
    "There are many ways to encode categorical variables for modeling, although the three most common are as follows:\n",
    "\n",
    "Integer Encoding: Where each unique label is mapped to an integer.\n",
    "One Hot Encoding: Where each label is mapped to a binary vector.\n",
    "Learned Embedding: Where a distributed representation of the categories is learned.\n",
    "We will take a closer look at how to encode categorical data for training a deep learning neural network in Keras using each one of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Ordinal Encode Categorical Data\n",
    "An ordinal encoding involves mapping each unique label to an integer value.\n",
    "\n",
    "As such, it is sometimes referred to simply as an integer encoding.\n",
    "\n",
    "This type of encoding is really only appropriate if there is a known relationship between the categories.\n",
    "\n",
    "This relationship does exist for some of the variables in the dataset, and ideally, this should be harnessed when preparing the data.\n",
    "\n",
    "In this case, we will ignore any possible existing ordinal relationship and assume all variables are categorical. It can still be helpful to use an ordinal encoding, at least as a point of reference with other encoding schemes.\n",
    "\n",
    "We can use the OrdinalEncoder() from scikit-learn to encode each variable to integers. This is a flexible class and does allow the order of the categories to be specified as arguments if any such order is known.\n",
    "\n",
    "Note: I will leave it as an exercise for you to update the example below to try specifying the order for those variables that have a natural ordering and see if it has an impact on model performance.\n",
    "\n",
    "The best practice when encoding variables is to fit the encoding on the training dataset, then apply it to the train and test datasets.\n",
    "\n",
    "The function below, named prepare_inputs(), takes the input data for the train and test sets and encodes it using an ordinal encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to One Hot Encode Categorical Data\n",
    "A one hot encoding is appropriate for categorical data where no relationship exists between categories.\n",
    "\n",
    "It involves representing each categorical variable with a binary vector that has one element for each unique label and marking the class label with a 1 and all other elements 0.\n",
    "\n",
    "For example, if our variable was “color” and the labels were “red,” “green,” and “blue,” we would encode each of these labels as a three-element binary vector as follows:\n",
    "\n",
    "Red: [1, 0, 0]\n",
    "Green: [0, 1, 0]\n",
    "Blue: [0, 0, 1]\n",
    "Then each label in the dataset would be replaced with a vector (one column becomes three). This is done for all categorical variables so that our nine input variables or columns become 43 in the case of the breast cancer dataset.\n",
    "\n",
    "The scikit-learn library provides the OneHotEncoder to automatically one hot encode one or more variables.\n",
    "\n",
    "The prepare_inputs() function below provides a drop-in replacement function for the example in the previous section. Instead of using an OrdinalEncoder, it uses a OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How would the learning activity be affected if certain variables have missing values? Having said\n",
    "that, what can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data present various problems. First, the absence of data reduces statistical power, which refers to the probability that the test will reject the null hypothesis when it is false. Second, the lost data can cause bias in the estimation of parameters. Third, it can reduce the representativeness of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Describe the various methods for dealing with missing data values in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Main Approaches to Compensate for Missing Values\n",
    "It generally cannot be determined whether data are missing at random, or whether the missingness depends on unobserved predictors or the missing data themselves.\n",
    "\n",
    "In practice, the missing at random assumption is reasonable.\n",
    "\n",
    "Several different approaches to imputing missing values are found in the literature:\n",
    "\n",
    "\n",
    "\n",
    "1. Imputation using zero, mean, median or most frequent value\n",
    "This works by imputing all missing values with zero, the mean or median for quantitative variables, or the most common value for categorical variables.\n",
    "\n",
    "Additionally, we can create a new variable that is an indicator of missingness and includes it in the model to predict the response. This is done after plugging in zero, mean, median, or most frequent value in the actual variable.\n",
    "\n",
    "2. Imputation using a randomly selected value\n",
    "This works by randomly selecting an observed entry in the variable and use it to impute missing values.\n",
    "\n",
    "3. Imputation with a model\n",
    "This works by replacing missing values with predicted values from a model based on the other observed predictors.\n",
    "\n",
    "The k nearest neighbor algorithm is often used to impute a missing value based on how closely it resembles the points in the training set.\n",
    "\n",
    "Model-based imputation with uncertainty works by replacing missing values with predicted values plus randomness from a model based on the other observed predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What are the various data pre-processing techniques? Explain dimensionality reduction and\n",
    "function selection in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Dimensionality Reduction?\n",
    "→ The term dimensionality reduction is often reserved for those techniques that reduce the dimensionality of a data set by creating new attributes that are a combination of the old attributes.\n",
    "Purpose:\n",
    "→ Avoid curse of dimensionality. To learn more about this, visit my earlier article explaining it in detail.\n",
    "→ Reduce amount of time and memory required by data mining algorithms.\n",
    "→ Allow data to be more easily visualised.\n",
    "→ May help to eliminate irrelevant features or reduce noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Feature Creation?\n",
    "→ It involves creation of new attributes that can capture the important information in a data set much more efficiently than the original attributes.\n",
    "Three general methodologies:\n",
    "Feature extraction\n",
    "→ The creation of a new set of features from the original raw data is known as feature extraction. Consider a set of photographs, where each photograph is to be classified according to whether or not it contains a human face. The raw data is a set of pixels, and as such, is not suitable for many types of classification algorithms. However, if the data is processed to provide higher- level features, such as the presence or absence of certain types of edges and areas that are highly correlated with the presence of human faces, then a much broader set of classification techniques can be applied to this problem.\n",
    "→ This method is highly domain specific.\n",
    "Feature construction\n",
    "→ Sometimes the features in the original data sets have the necessary information, but it is not in a form suitable for the data mining algorithm. In this situation, one or more new features constructed out of the original features can be more useful than the original features.\n",
    "→ Example: dividing mass by volume to get density\n",
    "Mapping data to new space\n",
    "→ A totally different view of the data can reveal important and interesting features. Consider, for example, time series data, which often contains periodic patterns. If there is only a single periodic pattern and not much noise then the pattern is easily detected. If, on the other hand, there are a number of periodic patterns and a significant amount of noise is present, then these patterns are hard to detect. Such patterns can, nonetheless, often be detected by applying a Fourier transform to the time series in order to change to a representation in which frequency information is explicit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\n",
    "\n",
    "i. What is the IQR? What criteria are used to assess it?\n",
    "\n",
    "ii. Describe the various components of a box plot in detail? When will the lower whisker\n",
    "surpass the upper whisker in length? How can box plots be used to identify outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IQR describes the middle 50% of values when ordered from lowest to highest. To find the interquartile range (IQR), ​first find the median (middle value) of the lower and upper half of the data. These values are quartile 1 (Q1) and quartile 3 (Q3). The IQR is the difference between Q3 and Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the median is in the middle of the box, and the whiskers are about the same on both sides of the box, then the distribution is symmetric. When the median is closer to the bottom of the box, and if the whisker is shorter on the lower end of the box, then the distribution is positively skewed (skewed right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Make brief notes on any two of the following:\n",
    "\n",
    "1. Data collected at regular intervals\n",
    "\n",
    "2. The gap between the quartiles\n",
    "\n",
    "3. Use a cross-tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interval data, also called an integer, is defined as a data type which is measured along a scale, in which each point is placed at equal distance from one another. Interval data always appears in the form of numbers or numerical values where the distance between the two points is standardized and equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 is the median (the middle) of the lower half of the data, and Q3 is the median (the middle) of the upper half of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Make a comparison between:\n",
    "\n",
    "1. Data with nominal and ordinal values\n",
    "\n",
    "2. Histogram and box plot\n",
    "\n",
    "3. The average and median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominal data is a group of non-parametric variables, while Ordinal data is a group of non-parametric ordered variables. Although, they are both non-parametric variables, what differentiates them is the fact that ordinal data is placed into some kind of order by their position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms and box plots are graphical representations for the frequency of numeric data values. ... Histograms are preferred to determine the underlying probability distribution of a data. Box plots on the other hand are more useful when comparing between several data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use three different types of average in maths: the mean, the mode and the median, each of which describes a different 'normal' value. The mean is what you get if you share everything equally, the mode is the most common value, and the median is the value in the middle of a set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
