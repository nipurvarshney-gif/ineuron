{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the basic architecture of RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network:\n",
    "The fundamental feature of a Recurrent Neural Network (RNN) is that the network contains at least one feed-back connection, so the activations can flow round in a loop. That enables the networks to do temporal processing and learn sequences, e.g., perform sequence recognition/reproduction or temporal association/prediction. Recurrent neural network architectures can have many different forms. One common type consists of a standard Multi-Layer Perceptron (MLP) plus added loops. These can exploit the powerful non-linear mapping capabilities of the MLP, and also have some form of memory. Others have more uniform structures, potentially with every neuron connected to all the others, and may also have stochastic activation functions. For simple architectures and deterministic activation functions, learning can be achieved using similar gradient descent procedures to those leading to the back-propagation algorithm for feed-forward networks.\n",
    " \n",
    "In sequential tasks such as natural language and speech processing, there is always dependence of present input data upon the previous applied inputs. Task of RNNs is to find the relationship between current input and the previous applied inputs. In theory RNNs can make use of information sequence of any arbitrarily length, but in practice they are limited to looking back only a few steps.\n",
    "Picture\n",
    "Figure: Basic architecture of Recurrent Neural Networks\n",
    " \n",
    "The above figure shows a RNN being unfolded into a full network. By unfolding we simply mean that we are repeating the same layer structure of network for the complete sequence.\n",
    "Xt is the input at time step t. Xt is a vector of any size N.\n",
    "A is the hidden state at time step t. It’s the “memory” of the network. It is calculated based on the previous hidden state and the input at the current step.\n",
    "Represented by At= f (W Xt +U At-1)\n",
    "Here W and U are weights for input and previous state value input. And f is the non-linearity applied to the sum to generate final cell state.\n",
    "\n",
    "One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky,” we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information. In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation Through Time\n",
    "Backpropagation Through Time, or BPTT, is the application of the Backpropagation training algorithm to recurrent neural network applied to sequence data like a time series.\n",
    "\n",
    "A recurrent neural network is shown one input each timestep and predicts one output.\n",
    "\n",
    "Conceptually, BPTT works by unrolling all input timesteps. Each timestep has one input timestep, one copy of the network, and one output. Errors are then calculated and accumulated for each timestep. The network is rolled back up and the weights are updated.\n",
    "\n",
    "Spatially, each timestep of the unrolled recurrent neural network may be seen as an additional layer given the order dependence of the problem and the internal state from the previous timestep is taken as an input on the subsequent timestep.\n",
    "\n",
    "We can summarize the algorithm as follows:\n",
    "\n",
    "Present a sequence of timesteps of input and output pairs to the network.\n",
    "Unroll the network then calculate and accumulate errors across each timestep.\n",
    "Roll-up the network and update weights.\n",
    "Repeat.\n",
    "BPTT can be computationally expensive as the number of timesteps increases.\n",
    "\n",
    "If input sequences are comprised of thousands of timesteps, then this will be the number of derivatives required for a single update weight update. This can cause weights to vanish or explode (go to zero or overflow) and make slow learning and model skill noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain Vanishing and exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two common problems that occur during the backpropagation of time-series data are the vanishing and exploding gradients. The equation above has two problematic cases:\n",
    "\n",
    "Image by Author\n",
    "In the first case, the term goes to zero exponentially fast, which makes it difficult to learn some long period dependencies. This problem is called the vanishing gradient. In the second case, the term goes to infinity exponentially fast, and their value becomes a NaN due to the unstable process. This problem is called the exploding gradient. In the following two sections, we review two approaches to deal with these problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain Long short-term memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory networks: \n",
    "Normally called as LSTMs, they are a special kind of RNN, capable of learning long-term dependencies. They work tremendously well on a large variety of problems, and are now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior. All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n",
    "In all traditional recurrent neural network, during the gradient back-propagation phase, the gradient signal can end up being multiplied a large number of times (as many as the number of time steps) by the weight matrix associated with the connections between the neurons of the recurrent hidden layer. This means that, the magnitude of weights in the transition matrix can have a strong impact on the learning process.\n",
    "If the weights in this matrix are small (or, more formally, if the leading eigenvalue of the weight matrix is smaller than 1.0), it can lead to a situation called vanishing gradients where the gradient signal gets so small that learning either becomes very slow or stops working altogether. It can also make more difficult the task of learning long-term dependencies in the data. Conversely, if the weights in this matrix are large (or, again, more formally, if the leading eigenvalue of the weight matrix is larger than 1.0), it can lead to a situation where the gradient signal is so large that it can cause learning to diverge. This is often referred to as exploding gradients.\n",
    "These issues are the main motivation behind the LSTM model which introduces a new structure called a memory cell . A memory cell is composed of four main elements: an input gate, a neuron with a self-recurrent connection (a connection to itself), a forget gate and an output gate. The self-recurrent connection has a weight of 1.0 and ensures that, barring any outside interference, the state of a memory cell can remain constant from one time step to another. The gates serve to modulate the interactions between the memory cell itself and its environment. The input gate can allow incoming signal to alter the state of the memory cell or block it. On the other hand, the output gate can allow the state of the memory cell to have an effect on other neurons or prevent it. Finally, the forget gate can modulate the memory cell’s self-recurrent connection, allowing the cell to remember or forget its previous state, as needed.\n",
    "​\n",
    "Picture\n",
    "Figure: A single cell of LSTM Network\n",
    "\n",
    "The three gates in LSTM are input gate [i], forget gate [f] and output gate [o]. The input to be updated in cell at current time step is [g].\n",
    "all of these gates and update signal are dependent on previous hidden state and the current input of the cell as per principle of recurrent nets.\n",
    "So,\n",
    "it = f ( Wi Xt + Ui ht-1 + bi )\n",
    "gt = tanh ( Wc Xt + Uc ht-1 + bc )\n",
    "ft = f ( Wf Xt + Uf ht-1 + bf )\n",
    "Now value to be updated in cell can be calculated using these three equations as\n",
    "Ct = it .gt + ft .Ct-1\n",
    "The output of this LSTM cell can be generated by using current cell state, current input and previous output. all these three signals are used to control output gate. According to the state of output gate, cell state is passed through the tanh function, just to keep output near zero and one.\n",
    "Ot = f ( Wo Xt + Uo ht-1 + Vo Ct + bo )\n",
    "ht = Ot . tanh( Ct)\n",
    "This LSTM cell is organised in same order as mnetioned in RNN architecture and size of the network depends on sequential dependency or relation between data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain Gated recurrent unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the Gated Recurrent Units (GRU) is one of the popular variants of recurrent neural networks and has been widely used in the context of machine translation. GRUs can also be regarded as a simpler version of LSTMs (Long Short-Term Memory). The GRU unit was introduced in 2014 and is claimed to be motivated by the Long Short-Term Memory unit. However, the former is much simpler to compute and implement in models.\n",
    "\n",
    "According to the researchers at the University of Montreal, a gated recurrent unit (GRU) was introduced to produce each recurrent unit to capture dependencies of various time scales. Similar to the Long Short-Term Memory unit, the GRU includes gating units which modulate the flow of information inside the unit without any separate memory cells.\n",
    "\n",
    "Gated recurrent unit networks as a variant of the recurrent neural network are able to process memories of sequential data by storing previous inputs in the internal state of networks and plan from the history of previous inputs to target vectors in principle. \n",
    "\n",
    "How It Works\n",
    "In GRU, two gates including a reset gate that adjusts the incorporation of new input with the previous memory and an update gate that controls the preservation of the precious memory are introduced. The reset gate and the update gate adaptively control how much each hidden unit remembers or forgets while reading/generating a sequence.\n",
    "\n",
    "\n",
    "In the above figure of the Gated Recurrent Unit, r and z are known to be the reset and update gates, while h and h˜ are the activations as well as the candidate activation respectively. The working of GRU proceeds such that when the reset gate is close to zero, the hidden state is forced to ignore the previous hidden state and is reset with the current input.\n",
    "\n",
    "SEE ALSO\n",
    "Recurrent Neural Network\n",
    "DEVELOPERS CORNER\n",
    "Implementing A Recurrent Neural Network (RNN) From Scratch\n",
    "This allows the hidden state to discard any data that is found to be irrelevant in the future. This result allows a more compact representation. While the update gate controls how much data from the previous hidden state will be transferred to the current hidden state. This process performs in a similar manner to the memory cell in the Long Short-Term Memory network and helps the RNN to remember long-term information.\n",
    "\n",
    "The activation of the GRU at a particular time is a linear interpolation between the previous activation and the candidate activation, where an update gate decides how much the unit updates its activation or content.\n",
    "\n",
    "Advantages of Gated Recurrent Unit\n",
    "Gated Recurrent Unit can be used to improve the memory capacity of a recurrent neural network as well as provide the ease of training a model. The hidden unit can also be used for settling the vanishing gradient problem in recurrent neural networks. It can be used in various applications, including speech signal modelling, machine translation, handwriting recognition, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Explain Peephole LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have seen simple LSTM network but this architecture is modified along with time in each and every research paper. One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.” This means that we let the gate layers look at the cell state.\n",
    "\n",
    "In this peephole connection we can see that all the gates are having an input along with the cell state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTMs are an extension of traditional LSTMs that can improve model performance on sequence classification problems.\n",
    "\n",
    "In problems where all timesteps of the input sequence are available, Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem.\n",
    "\n",
    "In this tutorial, you will discover how to develop Bidirectional LSTMs for sequence classification in Python with the Keras deep learning library.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "How to develop a small contrived and configurable sequence classification problem.\n",
    "How to develop an LSTM and Bidirectional LSTM for sequence classification.\n",
    "How to compare the performance of the merge mode used in Bidirectional LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Explain the gates of LSTM with equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forget Gate\n",
    "In a cell of the LSTM network, the first step is to decide whether we should keep the information from the previous timestamp or forget it. Here is the equation for forget gate.\n",
    "\n",
    "Forget Gate LSTM\n",
    "\n",
    "Let’s try to understand the equation, here\n",
    "\n",
    "Xt: input to the current timestamp.\n",
    "Uf: weight associated with the input\n",
    "Ht-1: The hidden state of the previous timestamp\n",
    "Wf: It is the weight matrix associated with hidden state\n",
    "Later, a sigmoid function is applied over it. That will make ft a number between 0 and 1. This ft is later multiplied with the cell state of the previous timestamp as shown below.\n",
    "\n",
    "timestamp\n",
    "\n",
    "If ft is 0 then the network will forget everything and if the value of ft is 1 it will forget nothing. Let’s get back to our example, The first sentence was talking about Bob and after a full stop, the network will encounter Dan, in an ideal case the network should forget about Bob.\n",
    "\n",
    " \n",
    "\n",
    "Input Gate\n",
    "Let’s take another example\n",
    "\n",
    "“Bob knows swimming. He told me over the phone that he had served the navy for four long years.”\n",
    "\n",
    "So, in both these sentences, we are talking about Bob. However, both give different kinds of information about Bob. In the first sentence, we get the information that he knows swimming. Whereas the second sentence tells he uses the phone and served in the navy for four years.\n",
    "\n",
    "Now just think about it, based on the context given in the first sentence, which information of the second sentence is critical. First, he used the phone to tell or he served in the navy. In this context, it doesn’t matter whether he used the phone or any other medium of communication to pass on the information. The fact that he was in the navy is important information and this is something we want our model to remember. This is the task of the Input gate.\n",
    "\n",
    "Input gate is used to quantify the importance of the new information carried by the input. Here is the equation of the input gate\n",
    "\n",
    "LSTM Input GateHere,\n",
    "\n",
    "Xt: Input at the current timestamp t\n",
    "Ui: weight matrix of input\n",
    "Ht-1: A hidden state at the previous timestamp\n",
    "Wi: Weight matrix of input associated with hidden state\n",
    "Again we have applied sigmoid function over it. As a result, the value of I at timestamp t will be between 0 and 1.\n",
    "\n",
    " \n",
    "\n",
    "New information\n",
    "New information LSTM\n",
    "\n",
    "Now the new information that needed to be passed to the cell state is a function of a hidden state at the previous timestamp t-1 and input x at timestamp t. The activation function here is tanh. Due to the tanh function, the value of new information will between -1 and 1. If the value is of Nt is negative the information is subtracted from the cell state and if the value is positive the information is added to the cell state at the current timestamp.\n",
    "\n",
    "However, the Nt won’t be added directly to the cell state. Here comes the updated equation\n",
    "\n",
    "new info\n",
    "\n",
    "Here, Ct-1 is the cell state at the current timestamp and others are the values we have calculated previously.\n",
    "\n",
    " \n",
    "\n",
    "Output Gate\n",
    "Now consider this sentence\n",
    "\n",
    "“Bob single-handedly fought the enemy and died for his country. For his contributions, brave________ .”\n",
    "\n",
    "During this task, we have to complete the second sentence. Now, the minute we see the word brave, we know that we are talking about a person. In the sentence only Bob is brave, we can not say the enemy is brave or the country is brave. So based on the current expectation we have to give a  relevant word to fill in the blank. That word is our output and this is the function of our Output gate.\n",
    "\n",
    "Here is the equation of the Output gate, which is pretty similar to the two previous gates.\n",
    "\n",
    "Output Gate\n",
    "\n",
    "Its value will also lie between 0 and 1 because of this sigmoid function. Now to calculate the current hidden state we will use Ot and tanh of the updated cell state. As shown below.\n",
    "\n",
    "Ot and tanh\n",
    "\n",
    "It turns out that the hidden state is a function of Long term memory (Ct) and the current output.  If you need to take the output of the current timestamp just apply the SoftMax activation on hidden state Ht.\n",
    "\n",
    "SoftMax\n",
    "\n",
    "Here the token with the maximum score in the output is the prediction.\n",
    "\n",
    "This is the More intuitive diagram of the LSTM network.\n",
    "\n",
    "LSTM network\n",
    "\n",
    "This diagram is taken from an interesting blog. I urge you all to go through it. Here is the link-\n",
    "\n",
    "Understanding LSTM Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Explain BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Explain BiGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
