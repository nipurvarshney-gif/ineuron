{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What exactly is a feature? Give an example to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the various circumstances in which feature construction is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above two school of thoughts co-exist, we’d focus here on the Motivation for Feature Selection.\n",
    "Especially when dealing with a large number of variables there is a need for Dimensionality Reduction\n",
    "Feature Selection can significantly improve a learning algorithm’s performance\n",
    "The Curse of Dimensionality\n",
    "\n",
    "The Curse of Dimensionality\n",
    "The required number of samples (to achieve the same accuracy) grows exponentially with the number of variables.\n",
    "In practice: the number of training examples is fixed.\n",
    "the classifier’s performance usually will degrade for a large number of features\n",
    "In many cases the information that is lost by discarding variables is made up for by a more accurate mapping/sampling in the lower-dimensional space.\n",
    "Feature Selection — Optimality?\n",
    "In theory, the goal is to find an optimal feature-subset (one that maximizes the scoring function).\n",
    "In real world applications this is usually not possible.\n",
    "For most problems it is computationally intractable to search the whole space of possible feature subsets\n",
    "One usually has to settle for approximations of the optimal subset\n",
    "Most of the research in this area is devoted to finding efficient search-heuristics\n",
    "\n",
    "Feature Selection and Feature Extraction\n",
    "Optimal Feature Subset:\n",
    "Often, the definition of optimal feature subset in terms of classifier’s performance\n",
    "The best one can hope for theoretically is the Bayes error rate\n",
    "Relevance of Features\n",
    "There are several definitions of relevance in literature.\n",
    "Relevance of 1 variable, Relevance of a variable given other variables, Relevance given a certain learning algorithm ,..\n",
    "Most definitions are problematic, because there are problems where all features would be declared to be irrelevant\n",
    "This can be defined through two degrees of relevance: weak and strong relevance.\n",
    "A feature is relevant iff it is weakly or strongly relevant and irrelevant (redundant) otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describe how nominal variables are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "One Hot Encoding –\n",
    "It refers to splitting the column which contains numerical categorical data to many columns depending on the number of categories present in that column. Each column contains “0” or “1” corresponding to which column it has been placed.\n",
    "\n",
    "For example :\n",
    "\n",
    "Consider the data where fruits and their corresponding categorical value and prices are given.\n",
    "\n",
    "Fruit\tCategorical value of fruit\tPrice\n",
    "apple\t1\t5\n",
    "mango\t2\t10\n",
    "apple\t1\t15\n",
    "orange\t3\t20\n",
    "The output after one hot encoding the data is given as follows,\n",
    "\n",
    "\n",
    "\n",
    "apple\tmango\torange\tprice\n",
    "1\t0\t0\t5\n",
    "0\t1\t0\t10\n",
    "1\t0\t0\t15\n",
    "0\t0\t1\t20\n",
    "Below is the Implementation in Python –\n",
    "\n",
    "Example 1:\n",
    "\n",
    "The following example is the data of zones and credit scores of customers, the zone is a categorical value which needs to be one hot encoded.\n",
    "\n",
    "# Program for demonstration of one hot encoding\n",
    "  \n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "  \n",
    "# import the data required\n",
    "data = pd.read_csv(r\"../../onehotenc_data.csv\")\n",
    "print(data)\n",
    "Output:\n",
    "data for example 1\n",
    "\n",
    "To one hot encode the zone column –\n",
    "\n",
    "# importing one hot encoder from sklearn\n",
    "# There are changes in OneHotEncoder class\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "   \n",
    "# creating one hot encoder object with categorical feature 0\n",
    "# indicating the first column\n",
    "columnTransformer = ColumnTransformer([('encoder',\n",
    "                                        OneHotEncoder(),\n",
    "                                        [0])],\n",
    "                                      remainder='passthrough')\n",
    "  \n",
    "data = np.array(columnTransformer.fit_transform(data), dtype = np.str)\n",
    "Output:\n",
    "\n",
    "\n",
    "The output contains 5 columns, one column for the price, and the remaining 4 columns representing the 4 zones.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Describe how numeric features are converted to categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "One-hot encoding\n",
    " \n",
    "There are several ways to encode categorical features (see, for example, here). In this post, we will focus on one of the most common and useful ones, one-hot encoding. After the transformation, each column of the resulting data set corresponds to one unique value of each original feature.\n",
    "\n",
    "For example, suppose we have the following categorical feature with three different unique values.\n",
    "\n",
    "+---------+\n",
    "| Feature |\n",
    "+---------+\n",
    "| value_1 |\n",
    "| value_2 |\n",
    "| value_3 |\n",
    "+---------+\n",
    "\n",
    "\n",
    "After one-hot encoding, the data set looks like:\n",
    "\n",
    "+-----------------+-----------------+-----------------+\n",
    "| Feature_value_1 | Feature_value_2 | Feature_value_3 |\n",
    "+-----------------+-----------------+-----------------+\n",
    "|               1 |               0 |               0 |\n",
    "|               0 |               1 |               0 |\n",
    "|               0 |               0 |               1 |\n",
    "+-----------------+-----------------+-----------------+\n",
    "\n",
    "\n",
    "We want to implement the one-hot encoding to the breast cancer data set, in such a way that the resulting sets are suitable to use in machine learning algorithms. Note that for many of the features of this data set there is a natural ordering between the categories (e.g. the tumour size) and, therefore, other types of encoding might be more appropriate, but for concreteness we will focus only on one-hot encoding in this post.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this\n",
    "approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wrapper methods\n",
    "In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset.\n",
    "\n",
    "It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The evaluation criterion is simply the performance measure which depends on the type of problem, for e.g. For regression evaluation criterion can be p-values, R-squared, Adjusted R-squared, similarly for classification the evaluation criterion can be accuracy, precision, recall, f1-score, etc. Finally, it selects the combination of features that gives the optimal results for the specified machine learning algorithm.\n",
    "\n",
    "wrapper feature selection\n",
    "Flow Chart – Wrapper methods\n",
    "Most commonly used techniques under wrapper methods are:\n",
    "\n",
    "Forward selection\n",
    "\n",
    "Backward elimination\n",
    "\n",
    "Bi-directional elimination(Stepwise Selection)\n",
    "\n",
    "Too much theory so far. Now let us discuss wrapper methods with an example of the Boston house prices dataset available in sklearn. The dataset contains 506 observations of 14 different features. The dataset can be imported using the load_boston()function available in the sklearn.datasets module.\n",
    "wrapper feature selection\n",
    "Here, the target variable is Price. We will be fitting a regression model to predict Price by selecting optimal features through wrapper methods.\n",
    "\n",
    "1. Forward selection\n",
    "In forward selection, we start with a null model and then start fitting the model with each individual feature one at a time and select the feature with the minimum p-value. Now fit a model with two features by trying combinations of the earlier selected feature with all other remaining features. Again select the feature with the minimum p-value. Now fit a model with three features by trying combinations of two previously selected features with other remaining features. Repeat this process until we have a set of selected features with a p-value of individual features less than the significance level.\n",
    "\n",
    "In short, the steps for the forward selection technique are as follows :\n",
    "\n",
    "Choose a significance level (e.g. SL = 0.05 with a 95% confidence).\n",
    "\n",
    "Fit all possible simple regression models by considering one feature at a time. Total ’n’ models are possible. Select the feature with the lowest p-value.\n",
    "\n",
    "Fit all possible models with one extra feature added to the previously selected feature(s).\n",
    "\n",
    "Again, select the feature with a minimum p-value. if p_value < significance level then go to Step 3, otherwise terminate the process.\n",
    "\n",
    "Now let us perform the same on Boston house price data.\n",
    "\n",
    "This above function accepts data, target variable, and significance level as arguments and returns the final list of significant features based on the wrapper methods based feature selection techniques. SequentialFeatureSelector() function comes with various combinations of feature selection techniques.\n",
    "\n",
    "#importing the necessary libraries\n",
    "from mlxtend.feature_selection import SequentialFeatureSelect\n",
    "SequentialFeatureSelector() function accepts the following major arguments :\n",
    "\n",
    "LinearRegression() is an estimator for the entire process. Similarly, it can be any classification based algorithm.\n",
    "\n",
    "k_features indicates the number of features to be selected. It can be any random value, but the optimal value can be found by analyzing and visualizing the scores for different numbers of features.\n",
    "\n",
    "forward and floating arguments for different flavors of wrapper methods, here, forward = True and floating = False are for forward selection technique.\n",
    "\n",
    "The scoring argument specifies the evaluation criterion to be used. For regression problems, there is only r2 score in default implementation. Similarly for classification, it can be accuracy, precision, recall, f1-score, etc.\n",
    "\n",
    "cv argument is for k-fold cross-validation.\n",
    "\n",
    "Now let’s fit the above-defined feature selector on the Boston house price dataset.\n",
    "\n",
    "2. Backward elimination\n",
    "In backward elimination, we start with the full model (including all the independent variables) and then remove the insignificant feature with the highest p-value(> significance level). This process repeats again and again until we have the final set of significant features.\n",
    "\n",
    "In short, the steps involved in backward elimination are as follows:\n",
    "\n",
    "Choose a significance level (e.g. SL = 0.05 with a 95% confidence).\n",
    "\n",
    "Fit a full model including all the features.\n",
    "\n",
    "Consider the feature with the highest p-value. If the p-value > significance level then go to Step 4, otherwise terminate the process.\n",
    "\n",
    "Remove the feature which is under consideration.\n",
    "\n",
    "Fit a model without this feature. Repeat the entire process from Step 3.\n",
    "\n",
    "Now let us perform the same on Boston house price data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. When is a feature considered irrelevant? What can be said to quantify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept is really straightforward: We measure the importance of a feature by calculating the increase in the model's prediction error after permuting the feature. A feature is \"important\" if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In perspective of storing data in databases, storing correlated features is somehow similar to storing redundant information which it may cause wasting of storage and also it may cause inconsistent data after updating or editing tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. When is a function considered redundant? What criteria are used to identify features that could\n",
    "be redundant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data redundancy occurs when the same piece of data is stored in two or more separate places and is a common occurrence in many businesses. As more companies are moving  away from siloed data to using a central repository to store information, they are finding that their database is filled with inconsistent duplicates of the same entry. Although it can be challenging to reconcile — or even benefit from — duplicate data entries, understanding how to reduce and track data redundancy efficiently can help mitigate long-term inconsistency issues for your business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes data redundancy happens by accident while other times it is intentional. Accidental data redundancy can be the result of a complex process or inefficient coding while intentional data redundancy can be used to protect data and ensure consistency — simply by leveraging the multiple occurrences of data for disaster recovery and quality checks.\n",
    "\n",
    "If data redundancy is intentional, it’s important to have a central field or space for the data. This allows you to easily update all records of redundant data when necessary. When data redundancy isn’t purposeful, it can lead to a variety of issues which we’ll discuss below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. What are the various distance measurements used to determine feature similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson introduces three common measures for determining how similar texts are to one another: city block distance, Euclidean distance, and cosine distanc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. State difference between Euclidean and Manhattan distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting feature of finite dimensional space is that it doesn't matter what norm we apply to the space, it's topologically the same.\n",
    "\n",
    "That being said it's sensible and convenient to use the Euclidean norm, because this is the only norm up (up to linear transformation\\choice of basis) that has an associated an inner product, which is essentially a generalisation of the notion of an angle.\n",
    "\n",
    "We can see this fairly easily by taking an orthonormal basis (by Gram-Schmidt) and seeing that for this basis our norm becomes the Euclidean norm.\n",
    "\n",
    "In essence then whenever we wish to consider angles between vectors we must be using some variant of the Euclidean norm, so we generally use by default.\n",
    "\n",
    "It is sometimes easier to prove things in other norms, however, and their equivalence allows us to swap freely between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Distinguish between feature transformation and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is feature extraction/selection?\n",
    "Straight to the point:\n",
    "\n",
    "Extraction: Getting useful features from existing data.\n",
    "Selection: Choosing a subset of the original pool of features.\n",
    "Feature extraction vs. feature selection\n",
    "Why must we apply feature extraction/selection?\n",
    "Feature extraction is a quite complex concept concerning the translation of raw data into the inputs that a particular Machine Learning algorithm requires. The model is the motor, but it needs fuel to work. Features must represent the information of the data in a format that will best fit the needs of the algorithm that is going to be used to solve the problem.\n",
    "\n",
    "While some inherent features can be obtained directly from raw data, we usually need derived features from these inherent features that are actually relevant to attack the underlying problem. A poor model fed with meaningful features will surely perform better than an amazing algorithm fed with low-quality features – “garbage in, garbage out”.\n",
    "\n",
    "Feature extraction fills this requirement: it builds valuable information from raw data – the features – by reformatting, combining, transforming primary features into new ones…  until it yields a new set of data that can be consumed by the Machine Learning models to achieve their goals.\n",
    "\n",
    "Feature selection, for its part, is a clearer task: given a set of potential features, select some of them and discard the rest. Feature selection is applied either to prevent redundancy and/or irrelevancy existing in the features or just to get a limited number of features to prevent from overfitting.\n",
    "\n",
    "Note that if features are equally relevant, we could perform PCA technique to reduce the dimensionality and eliminate redundancy if that was the case. Here we would be doing feature extraction, as we were transforming the primary features and not just selecting a subset of them.\n",
    "\n",
    "When should we apply feature extraction/selection?\n",
    "First of all, we have to take into account what kind of algorithm we are going to feed with the produced features. Abstraction skills, irrelevancy and redundancy sensitivities vary a lot depending on the specific Machine Learning technique. \n",
    "\n",
    "In general, a minimum of feature extraction is always needed. The unique case when we wouldn’t need any feature extraction is when our algorithm can perform feature extraction by itself as in the deep learning neural networks, that can get a low dimensional representation of high dimensional data (go in depth here). In spite of this, it must be pointed out that getting success is always easier with good features.\n",
    "\n",
    "We should apply feature selection, when there is a suspicion of redundancy or irrelevancy, since these affect the model accuracy or simply add noise at best. Sometimes, despite having relevant and non-redundant features, feature selection may be performed only to reduce the number of features, in order to favor interpretability and computing feasibility or to avoid the curse of dimensionality phenomena, i.e., too many features to describe not enough samples.  \n",
    "\n",
    "How to apply feature extraction/selection?\n",
    "The response is pretty well defined regarding feature selection, which is enclosed:\n",
    "\n",
    "Wrappers: a wrapper evaluates a specific model sequentially using different potential subsets of features to get the subset that best works in the end. They are highly costly and have a high chance of overfitting, but also a high chance of success, on the other hand. Learn more here.\n",
    "Filters: for a much faster alternative, filters do not test any particular algorithm, but rank the original features according to their relationship with the problem (labels) and just select the top of them. Correlation and mutual information are the most widespread criteria. There are many easy to use tools, like the feature selection sklearn package.\n",
    "Embedded: this group is made up of all the Machine Learning techniques that include feature selection during their training stage. LASSO is an example.   \n",
    "Feature selection: filter, wrapper, embedded\n",
    "However, things are not so clear when discussing feature extraction. There are no limits to the ways of creating features.  Drawing these meaningful features often means a great deal of extensive exploration that involves expertise, creativity, intuition and time, lots of time. And this is the reason why, whereas automatic feature selection is already here, there is no so much development in feature extraction.\n",
    "\n",
    "A feature extraction pipeline varies a lot depending on the primary data and the algorithm to use and it turns into something difficult to consider abstractly. This is an example:\n",
    "\n",
    "Feature extraction pipeline\n",
    "Furthermore, there is not a complete consensus regarding which of the above tasks take part in feature extraction in effect:\n",
    "\n",
    "What is feature construction? Sometimes it is used to refer to manual versus (automatic) feature extraction.\n",
    "\n",
    "What about feature learning? It is meant as an automatic feature extraction.\n",
    "\n",
    "And feature transformation? It usually denotes less sophisticated transformations over the features, like re-scaling data, bucketing, etc. Some people do not consider it proper feature extraction, thinking that extraction refers only to the most scientific part of the work to do.\n",
    "\n",
    "And preprocessing? Normally it is the name for tasks like organizing data, cleaning it, dealing with missing values, outliers, encoding categorical values, etc. Again not everyone considers this feature extraction, but a previous task.\n",
    "\n",
    "And feature engineering? Well, sometimes it is used as a synonym for feature extraction, although contrary to extraction, there seems to be a relatively universal consensus that engineering involves not only creativity constructions but pre-processing tasks and naïve transformations as well.\n",
    "\n",
    "And are these concepts related to data mining? Yes, of course, but… stop!!\n",
    "\n",
    "Perhaps it is too soon to try to label any tasks involved in the Machine Learning field and it is good enough just knowing what makes sense as an input to help our model to success until an automatic feature extraction came up with an alternative.\n",
    "\n",
    "While we wait, maybe less than we think, don’t take features for granted; most of the time problems are in the data, not in the algorithm. Good recipes need good ingredients, so take care of your features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Make brief notes on any two of the following:\n",
    "\n",
    "1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2,A hybrid feature selection method is proposed for classification in small sample size data sets. ... A few candidate feature subsets are generated since their number corresponds to the number of instances. • Cooperative feature subset search is proposed with a classifier algorithm for the wrapper step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.We usually start with K-Means clustering. After going through several tutorials and Medium stories you will be able to implement k-means clustering easily. But as you implement it, a question starts to bug your mind: how can we measure its goodness of fit? Supervised algorithms have lots of metrics to check their goodness of fit like accuracy, r-square value, sensitivity, specificity etc. but what can we calculate to measure the accuracy or goodness of our clustering technique? The answer to this question is Silhouette Coefficient or Silhouette score.\n",
    "Silhouette Coefficient:\n",
    "Silhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1.\n",
    "1: Means clusters are well apart from each other and clearly distinguished.\n",
    "0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
    "-1: Means clusters are assigned in the wrong way.\n",
    "An illustrative figure to show how Silhouette score is calculated Source: Author original image\n",
    "Image by author\n",
    "Silhouette Score = (b-a)/max(a,b)\n",
    "where\n",
    "a= average intra-cluster distance i.e the average distance between each point within a cluster.\n",
    "b= average inter-cluster distance i.e the average distance between all clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The receiver operating characteristic (ROC) curve is the plot that displays the full picture of trade-off between the sensitivity and (1- specificity) across a series of cut- off points. Area under the ROC curve is considered as an effective measure of inherent validity of a diagnostic tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
