{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In the sense of machine learning, what is a model? What is the best way to train a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model represents what was learned by a machine learning algorithm. The model is the “thing” that is saved after running a machine learning algorithm on training data and represents the rules, numbers, and any other algorithm-specific data structures required to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define adequately our problem (objective, desired outputs…).\n",
    "Gather data.\n",
    "Choose a measure of success.\n",
    "Set an evaluation protocol and the different protocols available.\n",
    "Prepare the data (dealing with missing values, with categorial values…).\n",
    "Spilit correctly the data.\n",
    "Differentiate between over and underfitting, defining what they are and explaining the best ways to avoid them.\n",
    "An overview of how a model learns.\n",
    "What is regularization and when is appropiate to use it.\n",
    "Develop a benchmark model.\n",
    "Choose an adequate model and tune it to get the best performance possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the sense of machine learning, explain the &quot;No Free Lunch&quot; theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The No Free Lunch Theorem is often thrown around in the field of optimization and machine learning, often with little understanding of what it means or implies. The theorem states that all optimization algorithms perform equally well when their performance is averaged across all possible problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describe the K-fold cross-validation mechanism in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold Cross Validation\n",
    "First I would like to introduce you to a golden rule — “Never mix training and test data”. Your first step should always be to isolate the test data-set and use it only for final evaluation. Cross-validation will thus be performed on the training set.\n",
    "\n",
    "5 Fold Cross-Validation\n",
    "Initially, the entire training data set is broken up in k equal parts. The first part is kept as the hold out (testing) set and the remaining k-1 parts are used to train the model. Then the trained model is then tested on the holdout set. The above process is repeated k times, in each case we keep on changing the holdout set. Thus, every data point get an equal opportunity to be included in the test set.\n",
    "Usually, k is equal to 3 or 5. It can be extended even to higher values like 10 or 15 but it becomes extremely computationally expensive and time-consuming. Let us have a look at how we can implement this with a few lines of Python code and the Sci-kit Learn API.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(model, X_train, y_train, cv=5))\n",
    "We pass the model or classifier object, the features, the labels and the parameter cv which indicates the K for K-Fold cross-validation. The method will return a list of k accuracy values for each iteration. In general, we take the average of them and use it as a consolidated cross-validation score.\n",
    "import numpy as np\n",
    "print(np.mean(cross_val_score(model, X_train, y_train, cv=5)))\n",
    "Although it might be computationally expensive, cross-validation is essential for evaluating the performance of the learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Describe the bootstrap sampling method. What is the aim of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is a statistical procedure that resamples a single dataset to create many simulated samples. This process allows you to calculate standard errors, construct confidence intervals, and perform hypothesis testing for numerous types of sample statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping is building a company from the ground up with nothing but personal savings, and with luck, the cash coming in from the first sales. The term is also used as a noun: A bootstrap is a business an entrepreneur with little or no outside cash or other support launches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the significance of calculating the Kappa value for a classification model? Demonstrate\n",
    "how to measure the Kappa value of a classification model using a sample collection of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n essence, the kappa statistic is a measure of how closely the instances classified by the machine learning classifier matched the data labeled as ground truth, controlling for the accuracy of a random classifier as measured by the expected accuracy.\n",
    "The kappa statistic is used to control only those instances that may have been correctly classified by chance. This can be calculated using both the observed (total) accuracy and the random accuracy. Kappa can be calculated as:\n",
    "Kappa = (total accuracy – random accuracy) / (1- random accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Class Metrics Made Simple, Part III: the Kappa Score (aka Cohen’s Kappa Coefficient)\n",
    "Measure The Agreement Between Predicted and True Values\n",
    "Boaz Shmueli\n",
    "Boaz Shmueli\n",
    "\n",
    "Dec 18, 2019·6 min read\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In my previous Multi-Class Metrics Made Simple posts, I wrote about Precision and Recall, as well as the F1-score. I received encouraging feedback from many readers. So first, thank you! In this post, I write about another popular measure: the kappa score. You might find the kappa score to be useful in your application.\n",
    "The kappa score is an interesting metric. Its origins are in the field of psychology: it is used for measuring the agreement between two human evaluators or raters (e.g., psychologists) when rating subjects (patients). It was later “appropriated” by the machine-learning community to measure classification performance. Also known as Cohen’s kappa coefficient, the kappa score is named after Jacob Cohen, an American statistician and psychologist who wrote the seminal paper on the topic. Other names for this metric include Cohen’s kappa and the kappa statistic. In this post, I simply use kappa score.\n",
    "I first explain what is agreement and how the kappa score is used for measuring agreement to provide the intuition behind this metric. I then move on to classification.\n",
    "What is Agreement?\n",
    "Imagine that we run the piano department in a prestigious music school (Juilliard comes to mind for some reason). It’s admissions season, and 25 nervous candidates are ready to demonstrate their skills on the beautiful Steinway grand piano. We ask two experienced professors — known as Professor A and Professor B — to rate each candidate as Accept, Waiting List (WL), or Reject.\n",
    "Here are the audition results for the 25 candidates:\n",
    "\n",
    "Each candidate now has two ratings, one from each professor. Naturally, the professors agree on some candidates and disagree on others. A high level of agreement between the professors increases our confidence that the ratings are reliable. A low level of agreement means that we cannot trust the ratings. The kappa score measures the degree of agreement between the two evaluators, also known as inter-rater reliability.\n",
    "To compute the kappa score, it is convenient to first summarize the ratings in a matrix:\n",
    "\n",
    "The columns show the ratings by professor A. The rows show the ratings by Professor B. The value in each cell is the number of candidates with the corresponding ratings by the two professors.\n",
    "A naïve way to measure agreement would be to look at the proportion of ratings that are in agreement between the two professors. Let’s call this number Agree. Computing Agree is easy: we just need to look at the sum of the values in the green cells (diagonal) and divide by the total number of students: In our case, 4+2+6=12 of the 25 ratings are in agreement, so:\n",
    "Agree=12/25 =0.48\n",
    "That’s it! We just found the perfect metric for agreement. Or did we? Well, our metric is not very sophisticated, since it doesn’t consider that agreements can also happen by chance. That’s where the kappa score comes in; the kappa score considers how much better the agreements are over and beyond chance agreements. Thus, in addition to Agree, the kappa formula also uses the expected proportion of chance agreements; let’s call this number ChanceAgree.\n",
    "We already know how to compute Agree. For the moment, let’s assume that we know the value of ChanceAgree. We combine the two numbers to compute the kappa score as follows\n",
    "KappaScore=(Agree-ChanceAgree)/(1-ChanceAgree)\n",
    "Note that the numerator calculates is the difference between Agree and ChanceAgree. If Agree=1, we have perfect agreement, corresponding to a matrix where all the non-diagonal (pink) cells are 0. In this case, the kappa score is 1, regardless of ChanceAgree. In contrast, if Agree=ChanceAgree, kappa is 0, signifying that the professors’ agreement is by chance. If Agree is smaller than ChanceAgree, the kappa score is negative, denoting that the degree of agreement is lower than chance agreement.\n",
    "Back to our example: in our case (as we calculate shortly), ChanceAgree is 0.3024. In other words, we expect about 30% of agreements to occur by chance, which translates to about 7–8 students (30% of 25 students). The professors agreed on 12 of the 25 students, and so the kappa score is positive:\n",
    "KappaScore = (Agree-ChanceAgree)/(1-ChanceAgree)\n",
    "= (0.48–0.3024)/(1–0.3024)\n",
    "= 0.2546\n",
    "In the next section, I explain how ChanceAgree is calculated.\n",
    "To calculate ChanceAgree, we first look at the probability of rating a student as Accept for each professor. Let’s take a look at the rating matrix again; for convenience, we added totals for the rows and columns:\n",
    "\n",
    "For Professor A, 4+1+1=6 of the 25 ratings were Accept; for Professor B, the number is 4+6+3=13. The probabilities for rating a student as Accept for Professor A and B are thus:\n",
    "ProbA(Accept) = 6/25=0.24\n",
    "ProbB(Accept)=13/25=0.52\n",
    "The probability that both professors agree on an Accept by chance is equal to the product of ProbA and ProbB:\n",
    "ChanceAgree(Accept)=ProbA(Accept) × ProbB(Accept)=0.24×0.52=0.1248\n",
    "Similarly, we calculate the agreement probabilities for WL and Reject:\n",
    "ProbA(WL)=10/25=0.4\n",
    "ProbB(WL)=3/25=0.12\n",
    "ChanceAgree(WL)=ProbA(WL)×ProbB(WL)=0.4×0.12=0.048\n",
    "ProbA(Reject)=9/25=0.36\n",
    "ProbB(Reject)=9/25=0.36\n",
    "ChanceAgree(Reject)=ProbA(Reject)×ProbB(Reject)=0.36×0.36=0.1188\n",
    "Summing up the above three probabilities, we get the probability that agreement on any of the events — Accept, WL, or Reject — happened by chance:\n",
    "ChanceAgree=\n",
    "= ChanceAgree(Accept)+ChanceAgree(WL)+ChanceAgree(Reject)\n",
    "= 0.1248+0.0480+0.1188=0.3024\n",
    "That’s it! We learned how to calculate ChanceAgree. Combined with Agree, we can now measure agreement using the kappa score. But how can we use it to measure classification performance?\n",
    "Using the Kappa Score for Classification\n",
    "To see how kappa score is used as a metric of classifier performance, let’s look again at the rating matrix:\n",
    "\n",
    "Well, you might have guessed it already: it’s pretty similar to a confusion matrix! Compare it to the confusion matrix we used in my other posts (Precision and Recall, F1-score), where we built a classifier that detects the in photos: “Cat”, “Hen”, or “Fish”. Notice the “deep” similarity? The cell values are exactly the same.\n",
    "\n",
    "To use kappa for classification, we can simply think of each rating as a class; in addition, the ratings by Professor A are the true/actual values; the ratings by Professor B are the predicted values (or vice versa — it doesn’t matter). In this incarnation, the kappa score measures the degree of agreement between the true values and the predicted values, which we use as the classifier’s performance. In our case, we can use the previously calculated kappa score, since the cell values are identical:\n",
    "KappaScore = 0.2546\n",
    "Since 1 means perfect agreement and 0 is chance agreement, we most often get a value that’s somewhere in between. If the value is less than 0, we’re doing worse than chance agreement (disagreement), so something is severely broken in the classifier.\n",
    "The kappa score can be calculated using Python’s scikit-learn library (R users can use the cohen.kappa() function, which is part of the psych library).\n",
    "Here is how I confirmed my calculation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Describe the model ensemble method. In machine learning, what part does it play?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model . To better understand this definition lets take a step back into ultimate goal of machine learning and model building\n",
    "I will largely utilize Decision Trees to outline the definition and practicality of Ensemble Methods (however it is important to note that Ensemble Methods do not only pertain to Decision Trees).\n",
    "\n",
    "A Decision Tree determines the predictive value based on series of questions and conditions. For instance, this simple Decision Tree determining on whether an individual should play outside or not. The tree takes several weather factors into account, and given each factor either makes a decision or asks another question. In this example, every time it is overcast, we will play outside. However, if it is raining, we must ask if it is windy or not? If windy, we will not play. But given no wind, tie those shoelaces tight because were going outside to play.\n",
    "\n",
    "Decision Trees can also solve quantitative problems as well with the same format. In the Tree to the left, we want to know wether or not to invest in a commercial real estate property. Is it an office building? A Warehouse? An Apartment building? Good economic conditions? Poor Economic Conditions? How much will an investment return? These questions are answered and solved using this decision tree.\n",
    "When making Decision Trees, there are several factors we must take into consideration: On what features do we make our decisions on? What is the threshold for classifying each question into a yes or no answer? In the first Decision Tree, what if we wanted to ask ourselves if we had friends to play with or not. If we have friends, we will play every time. If not, we might continue to ask ourselves questions about the weather. By adding an additional question, we hope to greater define the Yes and No classes.\n",
    "This is where Ensemble Methods come in handy! Rather than just relying on one Decision Tree and hoping we made the right decision at each split, Ensemble Methods allow us to take a sample of Decision Trees into account, calculate which features to use or questions to ask at each split, and make a final predictor based on the aggregated results of the sampled Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What is a descriptive model&#39;s main purpose? Give examples of real-world problems that\n",
    "descriptive models were used to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of descriptive research is to describe a phenomenon and its characteristics. This research is more concerned with what rather than how or why something has happened. Therefore, observation and survey tools are often used to gather data.\n",
    "Data scientists don’t solve “analytics problems”, they solve problems that can be solved by analytics. For instance, supply chain efficiency issues are often described as data science problems. Savi, a logistics firm, has even referred to data scientists as the “superheroes of the supply chain”; food companies, militaries, and high tech manufacturing firms are just a few examples of industries that have leveraged data science to improve supply chain efficiency.\n",
    "\n",
    "Corporations have long recognised an efficient supply chain as being an important element of cost control. Guaranteeing supply chain efficiency requires the ability to account for the various factors that can affect the speed at which products move through a supply chain; because these factors can be described as data, data scientists are able to use them to create models capable of accurately forecasting future scenarios. With modern supply chains becoming ever-more complex, businesses have come to increasingly rely on data scientists for this task.  This is particularly relevant when blockchain is applied to manage the provenance of highly valued products like precious gemstones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Describe how to evaluate a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression model, the most commonly known evaluation metrics include:\n",
    "\n",
    "R-squared (R2), which is the proportion of variation in the outcome that is explained by the predictor variables. In multiple regression models, R2 corresponds to the squared correlation between the observed outcome values and the predicted values by the model. The Higher the R-squared, the better the model.\n",
    "\n",
    "Root Mean Squared Error (RMSE), which measures the average error performed by the model in predicting the outcome for an observation. Mathematically, the RMSE is the square root of the mean squared error (MSE), which is the average squared difference between the observed actual outome values and the values predicted by the model. So, MSE = mean((observeds - predicteds)^2) and RMSE = sqrt(MSE). The lower the RMSE, the better the model.\n",
    "\n",
    "Residual Standard Error (RSE), also known as the model sigma, is a variant of the RMSE adjusted for the number of predictors in the model. The lower the RSE, the better the model. In practice, the difference between RMSE and RSE is very small, particularly for large multivariate data.\n",
    "\n",
    "Mean Absolute Error (MAE), like the RMSE, the MAE measures the prediction error. Mathematically, it is the average absolute difference between observed and predicted outcomes, MAE = mean(abs(observeds - predicteds)). MAE is less sensitive to outliers compared to RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Distinguish :\n",
    "\n",
    "1. Descriptive vs. predictive models\n",
    "\n",
    "2. Underfitting vs. overfitting the model\n",
    "\n",
    "3. Bootstrapping vs. cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descriptive vs. predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive analytics looks at data statistically to tell you what happened in the past. Descriptive analytics helps a business understand how it is performing by providing context to help stakeholders interpret information. This can be in the form of data visualizations like graphs, charts, reports, and dashboards.\n",
    "\n",
    "How can descriptive analytics help in the real world? In a healthcare setting, for instance, say that an unusually high number of people are admitted to the emergency room in a short period of time. Descriptive analytics tells you that this is happening and provides real-time data with all the corresponding statistics (date of occurrence, volume, patient details, etc.).\n",
    "Prescriptive analytics takes predictive data to the next level. Now that you have an idea of what will likely happen in the future, what should you do? It suggests various courses of action and outlines what the potential implications would be for each.\n",
    "\n",
    "Back to our hospital example: now that you know the illness is spreading, the prescriptive analytics tool may suggest that you increase the number of staff on hand to adequately treat the influx of patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Underfitting vs. overfitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is a modeling error which occurs when a function is too closely fit to a limited set of data points. Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "\n",
    "Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. Intuitively, underfitting occurs when the model or the algorithm does not fit the data well enough. Specifically, underfitting occurs if the model or algorithm shows low variance but high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Bootstrapping vs. cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstrap resamples with replacement (and usually produces new \"surrogate\" data sets with the same number of cases as the original data set). Due to the drawing with replacement, a bootstrapped data set may contain multiple instances of the same original cases, and may completely omit other original cases.\n",
    "cross validation resamples without replacement and thus produces surrogate data sets that are smaller than the original. These data sets are produced in a systematic way so that after a pre-specified number k of surrogate data sets, each of the n original cases has been left out exactly once. This is called k-fold cross validation or leave-x-out cross validation with x=nk, e.g. leave-one-out cross validation omits 1 case for each surrogate set, i.e. k=n.\n",
    "\n",
    "As the name cross validation suggests, its primary purpose is measuring (generalization) performance of a model. On contrast, bootstrapping is primarily used to establish empirical distribution functions for a widespread range of statistics (widespread as in ranging from, say, the variation of the mean to the variation of models in bagged ensemble models).\n",
    "\n",
    "The leave-one-out analogue of the bootstrap procedure is called jackknifing (and is actually older than bootstrapping).\n",
    "\n",
    "The bootstrap analogue to cross validation estimates of generalization error is called out-of-bootstrap estimate (because the test cases are those that were left out of the bootstrap resampled training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Make quick notes on:\n",
    "\n",
    "1. LOOCV.\n",
    "\n",
    "2. F-measurement\n",
    "\n",
    "3. The width of the silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOCV(Leave One Out Cross-Validation) is a type of cross-validation approach in which each observation is considered as the validation set and the rest (N-1) observations are considered as the training set. In LOOCV, fitting of the model is done and predicting using one observation validation set. Furthermore, repeating this for N times for each observation as the validation set. Model is fitted and the model is used to predict a value for observation. This is a special case of K-fold cross-validation in which the number of folds is the same as the number of observations(K = N). This method helps to reduce Bias and Randomness. The method aims at reducing the Mean-Squared error rate and prevent over fitting. It is very much easy to perform LOOCV in R programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an imbalanced classification problem with two classes, precision is calculated as the number of true positives divided by the total number of true positives and false positives.\n",
    "\n",
    "Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "The result is a value between 0.0 for no precision and 1.0 for full or perfect precision.\n",
    "\n",
    "Let’s make this calculation concrete with some examples.\n",
    "\n",
    "Consider a dataset with a 1:100 minority to majority ratio, with 100 minority examples and 10,000 majority class examples.\n",
    "\n",
    "A model makes predictions and predicts 120 examples as belonging to the minority class, 90 of which are correct, and 30 of which are incorrect.\n",
    "\n",
    "The precision for this model is calculated as:\n",
    "\n",
    "Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "Precision = 90 / (90 + 30)\n",
    "Precision = 90 / 120\n",
    "Precision = 0.75\n",
    "The result is a precision of 0.75, which is a reasonable value but not outstanding.\n",
    "\n",
    "You can see that precision is simply the ratio of correct positive predictions out of all positive predictions made, or the accuracy of minority class predictions.\n",
    "\n",
    "Consider the same dataset, where a model predicts 50 examples belonging to the minority class, 45 of which are true positives and five of which are false positives. We can calculate the precision for this model as follows:\n",
    "\n",
    "Precision = TruePositives / (TruePositives + FalsePositives)\n",
    "Precision = 45 / (45 + 5)\n",
    "Precision = 45 / 50\n",
    "Precision = 0.90\n",
    "In this case, although the model predicted far fewer examples as belonging to the minority class, the ratio of correct positive examples is much better.\n",
    "\n",
    "This highlights that although precision is useful, it does not tell the whole story. It does not comment on how many real positive class examples were predicted as belonging to the negative class, so-called false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The width of the silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
